{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previsão Idade PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste notebook será abordado o problema de estimação de idade, utilizando agora a técnica de PCA. Tal como nos notebooks passados, é preciso, numa fase inicial, carregar os dados dos ficheiros previamente criados. De seguida será feita a separação dos dados para treino/teste (com a respetiva passagem de pixeis para as componentes principais, reduzindo significativamente a dimensão do input) e por fim testar-se-á o sucesso da rede. Para tal usaremos não só a métrica da accuracy como de 1Off (qual a percentagem de acertar no bin correto ou num diretamente adjacente a este, +/-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports necessários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recuperação dos dados dos ficheiros relativamente às imagens/labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leitura do ficheiro csv correspondente a todas as labels e anexação destas à lista results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leitura do ficheiro csv correspondente a todas as labels\n",
    "#anexação destas a lista results\n",
    "import csv\n",
    "results = []\n",
    "with open(\"HalfData.csv\") as csvfile:\n",
    "    reader = csv.reader(csvfile, quoting=csv.QUOTE_ALL) # change contents to floats\n",
    "    for row in reader: # each row is a list\n",
    "        results.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processamento das idades e conversão destas para os respetivos bins.\n",
    "Tal como referido anteriormente estes bins seguem a distribuição normal dos dados do dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processamento das idades e conversão para lista\n",
    "labelslist = []\n",
    "label =\"\"\n",
    "for i in range(len(results)):\n",
    "    age = int(results[i][0])\n",
    "    if(age>=18 and age<=21):\n",
    "        label = \"18_21\"\n",
    "    if(age>=22 and age<=24):\n",
    "        label = \"22_24\"\n",
    "    if(age>=25 and age<=28):\n",
    "        label = \"25_28\"\n",
    "    if(age>=29 and age<=33):\n",
    "        label = \"29_33\"\n",
    "    if(age>=34 and age<=40):\n",
    "        label = \"34_40\"\n",
    "    if(age>=41 and age<=47):\n",
    "        label = \"41_47\"\n",
    "    if(age>=48 and age<=55):\n",
    "        label = \"48_55\"\n",
    "    if(age>=56 and age<=65):\n",
    "        label = \"56_65\"\n",
    "    labelslist.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['56_65', '22_24', '34_40', ..., '25_28', '48_55', '25_28'],\n",
       "      dtype='<U5')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.reshape(labelslist,len(labelslist))\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leitura do ficheiro relativo aos pontos das fotos.\n",
    "Neste caso foi utilizado o que possuía metade da informação do dataset (13877 fotos).\n",
    "Este foi guardado num ficheiro binário por uma questão de redução de dimensão (uma vez que este contém 13877 * 256 * 256 floats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = np.reshape(np.fromfile(\"HalfData\"),(13877,256,256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisão dos dados para treino e teste\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez que é necessária uma divisão do dataset para treino e teste, utilizamos o train_test_split com a flag de stratify (garantindo que a distribuição dos dados se mantem nas versões \"reduzidas\") e com random_state, permitindo assim que a operação se torne determinística (os mesmos dados irão ser divididos sempre para os mesmos conjuntos de treino e teste, enquanto que o valor desta flag se mantenha constante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(teste,labels, test_size=0.20, stratify=labels, random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tendo os dados separados para treino e teste, podemos aplicar a técnica de PCA. Deste modo, obtemos as principais componentes dos dados e serão estas passadas à rede.\n",
    "\n",
    "\n",
    "Decidimos incluir as primeiras 10 componentes, que possuem 63% dos dados (neste momento cada componente adiciona pouco mais de 1%...).\n",
    "Começamos por faze-lo para as de treino com fit_transform e de seguida, para teste, com transform apenas fazendo com que esta siga as mesmas propriedades que a de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)\n",
    "principalComponents = pca.fit_transform(np.reshape(X_train,(len(X_train),256*256)))\n",
    "principalDf = pd.DataFrame(data = principalComponents)\n",
    "             #, columns = ['principal component 1', 'principal component 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30319141, 0.09155268, 0.05868098, 0.0468638 , 0.0333811 ,\n",
       "       0.02709735, 0.02222755, 0.01742475, 0.01732365, 0.01343905])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Peso de cada uma das 10 componentes\n",
    "pesos=pca.explained_variance_ratio_\n",
    "pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.631182332008199"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Somatório do peso das primeiras 10 componentes\n",
    "info=np.cumsum(pesos)\n",
    "info[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA para os dados de teste\n",
    "from sklearn.decomposition import PCA\n",
    "principalComponents2 = pca.transform(np.reshape(X_test,(len(X_test),256*256)))\n",
    "principalDf2 = pd.DataFrame(data = principalComponents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construção das labels para a rede (aplicação de one-hot-encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.get_dummies(y_train)\n",
    "train_labels = train_labels.to_numpy()\n",
    "test_labels = pd.get_dummies(y_test)\n",
    "test_labels = test_labels.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação e treino da rede -> PCA 6layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez que os dados a passar a rede são as principal components das fotos, consideramos que esta rede tinha de ser composta por múltiplas camadas Dense, de modo a maximizar a aprendizagem da mesma.\n",
    "\n",
    "Esta possui uma camada inicial com 10 neurnios (dimensão do input -> 10 principal components) e uma final com 8 (número de classes existentes -> dimensão do output). De resto utiliza camadas variadas entre 128,256 e 512.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(10, activation='relu', input_shape=(10,)))\n",
    "network.add(layers.Dense(128, activation='relu'))\n",
    "network.add(layers.Dense(256, activation='relu'))\n",
    "network.add(layers.Dense(512, activation='relu'))\n",
    "network.add(layers.Dense(128, activation='relu'))\n",
    "network.add(layers.Dense(8, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "694/694 [==============================] - 2s 2ms/step - loss: 2.0846 - accuracy: 0.1693\n",
      "Epoch 2/20\n",
      "694/694 [==============================] - 2s 2ms/step - loss: 2.0375 - accuracy: 0.1786\n",
      "Epoch 3/20\n",
      "694/694 [==============================] - 2s 2ms/step - loss: 2.0288 - accuracy: 0.1805\n",
      "Epoch 4/20\n",
      "694/694 [==============================] - 2s 2ms/step - loss: 2.0235 - accuracy: 0.1864\n",
      "Epoch 5/20\n",
      "694/694 [==============================] - 2s 2ms/step - loss: 2.0198 - accuracy: 0.1870\n",
      "Epoch 6/20\n",
      "694/694 [==============================] - 2s 2ms/step - loss: 2.0153 - accuracy: 0.1903\n",
      "Epoch 7/20\n",
      "694/694 [==============================] - 2s 2ms/step - loss: 2.0154 - accuracy: 0.1907\n",
      "Epoch 8/20\n",
      "694/694 [==============================] - 2s 2ms/step - loss: 2.0107 - accuracy: 0.1886\n",
      "Epoch 9/20\n",
      "694/694 [==============================] - 2s 3ms/step - loss: 2.0090 - accuracy: 0.1896\n",
      "Epoch 10/20\n",
      "694/694 [==============================] - 2s 2ms/step - loss: 2.0050 - accuracy: 0.1978\n",
      "Epoch 11/20\n",
      "694/694 [==============================] - 2s 3ms/step - loss: 2.0063 - accuracy: 0.1956\n",
      "Epoch 12/20\n",
      "694/694 [==============================] - 2s 2ms/step - loss: 2.0010 - accuracy: 0.1979\n",
      "Epoch 13/20\n",
      "694/694 [==============================] - 2s 2ms/step - loss: 2.0026 - accuracy: 0.1973\n",
      "Epoch 14/20\n",
      "694/694 [==============================] - 2s 2ms/step - loss: 2.0014 - accuracy: 0.1940\n",
      "Epoch 15/20\n",
      "694/694 [==============================] - 2s 2ms/step - loss: 1.9986 - accuracy: 0.2018\n",
      "Epoch 16/20\n",
      "694/694 [==============================] - 2s 2ms/step - loss: 1.9982 - accuracy: 0.2027\n",
      "Epoch 17/20\n",
      "694/694 [==============================] - 2s 2ms/step - loss: 1.9958 - accuracy: 0.1993\n",
      "Epoch 18/20\n",
      "694/694 [==============================] - 2s 2ms/step - loss: 1.9948 - accuracy: 0.2021\n",
      "Epoch 19/20\n",
      "694/694 [==============================] - 2s 2ms/step - loss: 1.9915 - accuracy: 0.2028\n",
      "Epoch 20/20\n",
      "694/694 [==============================] - 2s 2ms/step - loss: 1.9924 - accuracy: 0.2033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2878f3ae588>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "network.fit(principalComponents, train_labels, epochs=20,batch_size=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após o treino da rede, podemos passar à análise do sucesso da mesma. Assim, verificaremos a sua accuracy para fase de teste e posterior valor para 1Off, 1OffByClass e AccuracybyClass. Estas últimas métricas permitiram-nos perceber o sucesso do insucesso da rede, verificando assim se quando esta erra, foi por achar que seria uma idade próxima(bin adjacente) ou se é um palpite \"completamente\" aleatório.\n",
    "\n",
    "Como podemos ver pelo valor abaixo, a accuracy em teste é bastante proxima à de treino, permitindo assim concluir que a rede efetivamente aprendeu (por menos que tenha sido) e não entrou em OF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87/87 [==============================] - 0s 2ms/step - loss: 2.0401 - accuracy: 0.1913\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(principalComponents2, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsBOF = network.predict(principalComponents2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos verificar abaixo, esta rede apesar de possuir uma baixíssima capacidade de estimar a idade (19%), acerta quase metade das vezes (49%) quando incluindo o bin adjacente (ou seja, esta erra mais por não conseguir aprender especificamente os parâmetros para cada bin, mas efetivamente aprende a distinguir entre eles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def oneOff(predictions,correct):\n",
    "    acc = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if(np.argmax(predictions[i])==np.argmax(correct[i])+1):\n",
    "            acc = acc + 1 ;\n",
    "        if(np.argmax(predictions[i])==np.argmax(correct[i])-1): \n",
    "            acc = acc + 1 ;\n",
    "        if(np.argmax(predictions[i])==np.argmax(correct[i])):\n",
    "            acc = acc + 1 ;\n",
    "    return (acc/len(predictions))*100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49.92795389048991"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneOff(predictionsBOF,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Off By Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma vez que o valor de One off foi tão \"bom\" comparando à accuracy geral, consideramos que seria relevanta a sua análise por bin. Aqui surge um pormenor interessante, que é o valor \"altíssimo\" de precisão nos bins de 22 a 33 (maior concentração dos dados). Assim consideramos relevante, de modo a perceber melhor o porque desta especificidade, implementar também a accuracy por bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneOfbyClass(predictions,correct):\n",
    "    acc = [0,0,0,0,0,0,0,0]\n",
    "    listaLabels = [\"18_21\",\"22_24\",\"25_28\",\"29_33\",\"34_40\",\"41_47\",\"48_55\",\"56_65\"]\n",
    "        \n",
    "    for i in range(len(predictions)):\n",
    "        if(np.argmax(predictions[i])==np.argmax(correct[i])+1):\n",
    "            acc[np.argmax(correct[i])] = acc[np.argmax(correct[i])] + 1 ;\n",
    "        if(np.argmax(predictions[i])==np.argmax(correct[i])-1): \n",
    "            acc[np.argmax(correct[i])] = acc[np.argmax(correct[i])] + 1 ;\n",
    "        if(np.argmax(predictions[i])==np.argmax(correct[i])):\n",
    "            acc[np.argmax(correct[i])] = acc[np.argmax(correct[i])] + 1 ;\n",
    "    \n",
    "    listaIndices = [0,0,0,0,0,0,0,0]\n",
    "    for i in range(len(correct)):\n",
    "        listaIndices[np.argmax(correct[i])] = listaIndices[np.argmax(correct[i])] + 1\n",
    "    \n",
    "    newlista = []\n",
    "    for i in range(len(listaIndices)):\n",
    "        newlista.append(((acc[i]/listaIndices[i])*100,listaLabels[i]))\n",
    "        \n",
    "    \n",
    "    \n",
    "    return newlista;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2.05761316872428, '18_21'),\n",
       " (79.1044776119403, '22_24'),\n",
       " (83.08351177730194, '25_28'),\n",
       " (91.1832946635731, '29_33'),\n",
       " (18.341708542713565, '34_40'),\n",
       " (29.283489096573206, '41_47'),\n",
       " (24.679487179487182, '48_55'),\n",
       " (33.82899628252788, '56_65')]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oneOfbyClass(predictionsBOF,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy By Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal como podemos ver pelos valores abaixo, o motivo de valores tão grandes de accuracy nesses bins é devido ao facto de uma enorme precisão para o bin dos 25_28 (bin mais denso). Deste modo concluímos que a rede, quando não é capaz de identificar características específicas de um modelo, adivinha o que constitui o bin mais denso (mais provável de acertar), o que por uma questão de 1Off torna os bins adjacentes bastantes precisos nesta métrica, quando em accuracy absoluta são bastante \"fracos\" (1.7% e 0.23%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accByClass(predictions,correct):\n",
    "    acc = [0,0,0,0,0,0,0,0]\n",
    "    listaLabels = [\"18_21\",\"22_24\",\"25_28\",\"29_33\",\"34_40\",\"41_47\",\"48_55\",\"56_65\"]\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        if(np.argmax(predictions[i])==np.argmax(correct[i])):\n",
    "            acc[np.argmax(correct[i])] = acc[np.argmax(correct[i])] + 1 ;\n",
    "    \n",
    "    listaIndices = [0,0,0,0,0,0,0,0]\n",
    "    for i in range(len(correct)):\n",
    "        listaIndices[np.argmax(correct[i])] = listaIndices[np.argmax(correct[i])] + 1\n",
    "    \n",
    "    newlista = []\n",
    "    for i in range(len(listaIndices)):\n",
    "        newlista.append(((acc[i]/listaIndices[i])*100,listaLabels[i]))\n",
    "    \n",
    "    return newlista;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.411522633744856, '18_21'),\n",
       " (1.791044776119403, '22_24'),\n",
       " (80.08565310492506, '25_28'),\n",
       " (0.23201856148491878, '29_33'),\n",
       " (15.326633165829145, '34_40'),\n",
       " (3.115264797507788, '41_47'),\n",
       " (15.064102564102564, '48_55'),\n",
       " (11.524163568773234, '56_65')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accByClass(predictionsBOF,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
